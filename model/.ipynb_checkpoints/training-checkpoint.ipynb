{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b3e4d0-dfb0-4a0a-839f-e4bac83ac725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from time import sleep\n",
    "import tqdm\n",
    "from progressbar import progressbar\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "#import lightgbm as lgbm\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "from math import sqrt\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_contour\n",
    "from optuna.visualization import plot_edf\n",
    "from optuna.visualization import plot_intermediate_values\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "from optuna.visualization import plot_param_importances\n",
    "from optuna.visualization import plot_slice\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "import matplotlib.ticker as mtick\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append('/Users/bethbarlow/Documents/Nanodegree/udacity_ds_nanodegree_capstone')\n",
    "\n",
    "from utils.helpers import plot_feature_importances\n",
    "\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8673cfd-18a0-4ca6-bb62-4dc651b6d2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('/Users/bethbarlow/Documents/Nanodegree/udacity_ds_nanodegree_capstone/data/data_preproc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f57d22f9-612d-4b42-acd9-e7f39ae31f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df.drop(columns = 'Unnamed: 0', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa4483d-ea39-4bf4-9a97-683295b2e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(df.Sector)\n",
    "df.Sector = le.transform(df.Sector) # encode sector\n",
    "le.fit(df.Rating)\n",
    "df.Rating = le.transform(df.Rating) # encode rating\n",
    "le.fit(df['Rating Agency Name'])\n",
    "df['Rating Agency Name'] = le.transform(df['Rating Agency Name']) # encode rating agency name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d116601e-0655-492b-8001-4cb0f48575c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb82a76c-e638-4a55-aeb3-c5e934191ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2026, 28), (2026,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b2fc4ad-6622-42a9-b3a8-0e7831fd53f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=0, n=398 (19.645%)\n",
      "Class=4, n=670 (33.070%)\n",
      "Class=1, n=94 (4.640%)\n",
      "Class=3, n=490 (24.186%)\n",
      "Class=2, n=302 (14.906%)\n",
      "Class=5, n=72 (3.554%)\n"
     ]
    }
   ],
   "source": [
    "# summarize distribution\n",
    "counter = Counter(y)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y) * 100\n",
    "    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "    \n",
    "# plot the distribution\n",
    "#plt.bar(counter.keys(), counter.values())\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e50fb2-5a9a-4d86-bbf6-01c1cf048739",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_ix = df.iloc[:, 3:23].columns.tolist()\n",
    "categorical_ix = ['Rating Agency Name', 'Sector']\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown = 'ignore')\n",
    "\n",
    "X_encoded = pd.DataFrame(encoder.fit_transform(X[categorical_ix]).toarray())\n",
    "X_encoded.columns = encoder.get_feature_names_out(categorical_ix)\n",
    "X = X.join(X_encoded)\n",
    "X.drop(columns = categorical_ix, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0241a07a-ff5a-4d92-9805-4c68e85177fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Day_cos\n- Day_sin\n- Month_cos\n- Month_sin\n- Rating Agency Name_0\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/pp/yp7ffdpd6x98ps00y1b42jjm0000gn/T/ipykernel_71833/1422028288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumerical_ix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumerical_ix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# check - do you scale cyclical variables?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    509\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mvalidated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \"\"\"\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"requires_y\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    460\u001b[0m                 )\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     def _validate_data(\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Day_cos\n- Day_sin\n- Month_cos\n- Month_sin\n- Rating Agency Name_0\n- ...\n"
     ]
    }
   ],
   "source": [
    "# create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1/5)\n",
    "\n",
    "# scale numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X_train[numerical_ix] = scaler.fit_transform(X_train[numerical_ix]) # check - do you scale cyclical variables?\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e41208-1a97-4f77-9d86-ea80fad63beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 1/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0fbada-41db-4654-9f91-17016b9b51c2",
   "metadata": {},
   "source": [
    "### Fit Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a060b3a6-eeec-480f-9602-106e104968e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47831e8-2487-4de1-acc0-72a945812232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_get_predictions(classifier, features):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fits a model and gets predictions and execution times for train and test sets\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    trained_model = classifier.fit(X_train, y_train)\n",
    "    training_execution_time = time.time() - start_time\n",
    "                              \n",
    "    trained_model.feature_names = features\n",
    "    \n",
    "    # get predictions\n",
    "    train_preds = classifier.predict(X_train)\n",
    "    train_classification = classification_report(y_train, train_preds)\n",
    "    \n",
    "    train_f1 = f1_score(y_train, train_preds, average = 'weighted')\n",
    "\n",
    "    start_time = time.time()\n",
    "    val_preds = classifier.predict(X_val)\n",
    "    prediction_execution_time = time.time() - start_time\n",
    "    \n",
    "    val_f1 = f1_score(y_val, val_preds, average = 'weighted')\n",
    "    \n",
    "    # get model and predictions dictionary\n",
    "    model_and_predictions_dictionary = {\n",
    "        \n",
    "        'classifier': classifier,\n",
    "        'train_f1': train_f1,\n",
    "        'val_f1': val_f1,\n",
    "        'training_execution_time': np.round(training_execution_time, 0),\n",
    "        'prediction_execution_time': np.round(prediction_execution_time, 0)\n",
    "        \n",
    "    }\n",
    "    \n",
    "    return model_and_predictions_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dc3d03-6d1a-4ea2-b6dd-180a065a1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aacbf2-83b4-4766-a2e6-7ac1c673635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d5850-808a-40b2-b9c1-8bf7bb1da24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multinomial logistic regression\n",
    "lr = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter = 1000)\n",
    "\n",
    "lr_results = fit_model_get_predictions(lr, features)\n",
    "lr_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e670ac2-6f4f-42f3-a2e5-30d09f9a0aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest\n",
    "rf = RandomForestClassifier(RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 42))\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b10f5-1de9-4d88-b14f-845dd82aac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "knn = KNeighborsClassifier()\n",
    "knn_results = fit_model_get_predictions(knn, features)\n",
    "knn_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f7719-f3cc-445b-a614-569b6290b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive bayes\n",
    "gnb = GaussianNB()\n",
    "\n",
    "gnb_results = fit_model_get_predictions(gnb, features)\n",
    "gnb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe44e0d-1e89-4a48-aeaf-5c02bc150b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost \n",
    "xgb = XGBClassifier()\n",
    "xgb_results = fit_model_get_predictions(xgb, features)\n",
    "xgb_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f4eefb-6bec-4494-aa16-2a60e1fb3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_f1_list = [lr_results['val_f1'], knn_results['val_f1'], gnb_results['val_f1'], xgb_results['val_f1']]\n",
    "\n",
    "model_list = ['Logistic Regression', 'KNN', 'Naive Bayes', 'XGBoost']\n",
    "\n",
    "df_f1 = pd.DataFrame({'Model': model_list, 'F1 Score': val_f1_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8166c8b-eda7-4ad0-bef6-33e63e277222",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = list(df_f1.sort_values('F1 Score', ascending=False).Model)\n",
    "df_f1 = df_f1.sort_values('F1 Score', ascending=False).reset_index().drop(['index'], axis=1)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "# make barplot and sort bars\n",
    "x = sns.barplot(x='Model', y=\"F1 Score\", data=df_f1, order = order, palette=\"rocket\")\n",
    "plt.xlabel(\"Model\", fontsize=20)\n",
    "plt.ylabel(\"F1 Score\", fontsize=20)\n",
    "plt.title(\"F1 Score by Model\", fontsize=20)\n",
    "plt.grid(linestyle='-', linewidth='0.5', color='grey')\n",
    "plt.xticks(rotation=70, fontsize=12)\n",
    "plt.ylim(0,1)\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "for i in range(len(model_list)):\n",
    "    plt.text(x = i, y = df_f1.loc[i, 'F1 Score'] + 0.05, s = str(round((df_f1.loc[i, 'F1 Score'])*100, 2))+'%', \n",
    "             fontsize = 14, color='black',horizontalalignment='center')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25619c5-1022-4ca1-91f7-dfdc69fa577f",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35225f8a-2bfc-418b-bb8f-9411572168c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.concat([y_val, X_val], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc21c1a-4df8-4551-b282-b95a84c96a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "cv_results = {}\n",
    "confusion_matrix_plots = []\n",
    "scores = {\"AUC\" : []}\n",
    "models = {}\n",
    "seed = 1\n",
    "sampler_type = \"RandomUnderSampler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f173e0c4-519a-4103-869f-d75a44a609b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define objective function for optuna to maximize\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes a trial object as input and returns score\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 9),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500, 50),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "        'eval_metric': 'auc',\n",
    "    }\n",
    "    \n",
    "    clf = XGBClassifier(**params)\n",
    "        \n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train_res, y_train_res)\n",
    "    training_execution_time = time.time() - start_time\n",
    "    \n",
    "    preds = np.rint(clf.predict(X_val))\n",
    "    val_df['predictions'] = preds\n",
    "        \n",
    "    f1 = metrics.f1_score(y_val, preds, average = 'micro')\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e58414-f026-4c87-8f4c-aea86a8eefc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "sampler_types = [\"RandomUnderSampler\", \"RandomOverSampler\", \"SMOTE\"]\n",
    "optim_results = {}\n",
    "best_params = {}\n",
    "sampling_folds = 3\n",
    "n_trials = 30\n",
    "        \n",
    "# Loop over sampling types:\n",
    "for sampler_type in sampler_types:\n",
    "\n",
    "    # Optimise parameters for all three samplers\n",
    "    study = optuna.create_study(direction = 'maximize')\n",
    "\n",
    "    # Get a new sample after n trials are completed and repeat for n folds\n",
    "    for i in range(sampling_folds):\n",
    "\n",
    "        # Define sampler\n",
    "        sampler = eval(sampler_type)()\n",
    "\n",
    "        # Get a new sample\n",
    "        X_train_res, y_train_res = sampler.fit_resample(X_train, y_train)\n",
    "\n",
    "        study.optimize(objective, n_trials = n_trials)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        best_score = study.best_value\n",
    "        print(f\"Sampler: {sampler_type}\\n\")\n",
    "        print(f\"Best score: {best_score}\\n\")\n",
    "        print(f\"Optimized parameters: {best_params}\\n\")\n",
    "\n",
    "    print(\"Number of finished trials:\", len(study.trials))\n",
    "\n",
    "    trial = study.best_trial\n",
    "    for key, value in trial.params.items():\n",
    "        print('    {}: {}'.format(key, value))\n",
    "\n",
    "    best_params[sampler_type] = trial.params\n",
    "\n",
    "    optim_results[sampler_type] = study\n",
    "\n",
    "#     plot_optimization_history(study).show()\n",
    "#     plot_contour(study).show()\n",
    "#     plot_param_importances(study).show()\n",
    "\n",
    "optimisation_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750930c-786c-4174-80c2-2e38c127e973",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_summary = pd.DataFrame(columns = \n",
    "                               [\"Sampler\",\n",
    "                               \"Trial\",\n",
    "                               \"f1-score\",\n",
    "                               \"Parameters\"])\n",
    "\n",
    "# One study for each sampler, n trials within each study. Take best trial.\n",
    "for sampler, study in optim_results.items():\n",
    "    \n",
    "    tuning_summary.loc[len(tuning_summary.index)] = [\n",
    "        sampler,\n",
    "        int(study.best_trial.number),\n",
    "        study.best_trial.values[0],\n",
    "        study.best_trial.params,\n",
    "    ]\n",
    "\n",
    "    tuning_summary['Trial'] = tuning_summary['Trial'].astype(int)\n",
    "                            \n",
    "time_taken = optimisation_time/60\n",
    "tuning_summary = tuning_summary.round(4)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95275544-6e1c-438f-ab02-f309611e145f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593a1751-71b0-45d6-a69c-9eec9135b1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Re-train with tuned hyperparameters\n",
    "\n",
    "# best model is XGBoost\n",
    "%time\n",
    "\n",
    "# Combine train and validation sets to train model on full training set\n",
    "X_train = pd.concat([X_train, X_val], axis = 0)\n",
    "y_train = pd.concat([y_train, y_val], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fb0da-311a-4f18-8236-3e84849257e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eba974-bd0d-4d5d-8c97-c351f2765384",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\"f1\" : []}\n",
    "models, model_metrics = {}, {}\n",
    "seed = 1\n",
    "n_splits = 3\n",
    "sampling_strategies = [\"RandomUnderSampler\", \"RandomOverSampler\", \"SMOTE\"]\n",
    "tuned_params = dict(zip(tuning_summary.Sampler, tuning_summary.Parameters))\n",
    "\n",
    "for strategy in sampling_strategies:\n",
    "        \n",
    "        model_name = \"{} + XGBoost\".format(strategy)\n",
    "            \n",
    "        eval_result = {}\n",
    "            \n",
    "        clf = XGBClassifier(**tuned_params[strategy])\n",
    "            \n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = clf.predict(X_test)\n",
    "        \n",
    "        f1 = f1_score(y_test, predictions, average = 'micro')\n",
    "        \n",
    "        models[model_name] = clf\n",
    "        model_metrics[model_name] = f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfa7081-33bd-44c4-b25a-c9aa56cb8fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to log\n",
    "\n",
    "for idx, model in models.items():\n",
    "    \n",
    "    print(idx)\n",
    "    y_pred = model.predict(X_test)\n",
    "    report_dict = metrics.classification_report(y_test, y_pred, output_dict = True)\n",
    "\n",
    "    display(pd.DataFrame(report_dict).T.round(decimals = 2))\n",
    "\n",
    "    \n",
    "#     # Create a dictionary of desired metrics to write to a dataframe\n",
    "#     df_list.append(\n",
    "#         {\n",
    "#             \"Model\" : idx,\n",
    "#             \"Precision\" : report_dict.get(\"precision\"),\n",
    "#             \"Recall\" : report_dict.get(\"recall\"),\n",
    "#             \"Accuracy\" : report_dict.get(\"accuracy\")\n",
    "            \n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "# model_results = pd.DataFrame(df_list)\n",
    "# model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dfa0e6-9a59-46f6-af2a-dfbac5a13501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importances\n",
    "for idx, model in models.items():\n",
    "    plot_feature_importances(model, idx, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83189bed-f4fb-4b45-a50d-4cd334aaca48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
